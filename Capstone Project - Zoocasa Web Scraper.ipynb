{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "import geocoder\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import requests\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chromedriver = \"D:\\Downloads\\chromedriver_win32\\chromedriver.exe\" # path to the chromedriver executable\n",
    "chromedriver = os.path.expanduser(chromedriver)\n",
    "\n",
    "sys.path.append(chromedriver)\n",
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zoocasa_url = \"https://www.zoocasa.com/search?latitude=43.724004651806176&longitude=-79.39055077202798&zoom=12\"\n",
    "driver.get(zoocasa_url)\n",
    "\n",
    "signin = driver.find_elements_by_link_text(\"Sign in\")[0]\n",
    "signin.click()\n",
    "username = driver.find_element_by_id(\"text-input-email-730296\")\n",
    "username.clear()\n",
    "username.send_keys(\"yhchen518@gmail.com\")\n",
    "password = driver.find_element_by_id(\"text-input-password-76721\")\n",
    "password.clear()\n",
    "password.send_keys(\"danielchen84\")\n",
    "signpw = driver.find_elements_by_tag_name(\"button\")[-1]\n",
    "signpw.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login to zoocasa website in order to access more information of estates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zoocasa_url = \"https://www.zoocasa.com\"\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(zoocasa_url)\n",
    "\n",
    "uid=\"\"\n",
    "pwd=\"\"\n",
    "\n",
    "time.sleep(5)\n",
    "signin = driver.find_elements_by_link_text(\"Sign in\")[0]\n",
    "signin.click()\n",
    "\n",
    "time.sleep(5)\n",
    "username = driver.find_element_by_id(\"text-input-email-166647\")\n",
    "username.clear()\n",
    "username.send_keys(uid)\n",
    "\n",
    "password = driver.find_element_by_id(\"text-input-password-76721\")\n",
    "password.clear()\n",
    "password.send_keys(pwd)\n",
    "\n",
    "time.sleep(5)\n",
    "signpw = driver.find_elements_by_tag_name(\"button\")[-1]\n",
    "signpw.click()\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "pages = str(soup.find_all(\"em\")[0]).lstrip(\"<em>\").rstrip(\"</em>\")\n",
    "pages = int(pages[pages.index('f')+2::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start to do the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is scriping at  1 /72 pages...\n",
      "It is scriping at  2 /72 pages...\n",
      "It is scriping at  3 /72 pages...\n",
      "It is scriping at  4 /72 pages...\n",
      "It is scriping at  5 /72 pages...\n",
      "It is scriping at  6 /72 pages...\n",
      "It is scriping at  7 /72 pages...\n",
      "It is scriping at  8 /72 pages...\n",
      "It is scriping at  9 /72 pages...\n",
      "It is scriping at  10 /72 pages...\n",
      "It is scriping at  11 /72 pages...\n",
      "It is scriping at  12 /72 pages...\n",
      "It is scriping at  13 /72 pages...\n",
      "It is scriping at  14 /72 pages...\n",
      "It is scriping at  15 /72 pages...\n",
      "It is scriping at  16 /72 pages...\n",
      "It is scriping at  17 /72 pages...\n",
      "It is scriping at  18 /72 pages...\n",
      "It is scriping at  19 /72 pages...\n",
      "It is scriping at  20 /72 pages...\n",
      "It is scriping at  21 /72 pages...\n",
      "It is scriping at  22 /72 pages...\n",
      "It is scriping at  23 /72 pages...\n",
      "It is scriping at  24 /72 pages...\n",
      "It is scriping at  25 /72 pages...\n",
      "It is scriping at  26 /72 pages...\n",
      "It is scriping at  27 /72 pages...\n",
      "It is scriping at  28 /72 pages...\n",
      "It is scriping at  29 /72 pages...\n",
      "It is scriping at  30 /72 pages...\n",
      "It is scriping at  31 /72 pages...\n",
      "It is scriping at  32 /72 pages...\n",
      "It is scriping at  33 /72 pages...\n",
      "It is scriping at  34 /72 pages...\n",
      "It is scriping at  35 /72 pages...\n",
      "It is scriping at  36 /72 pages...\n",
      "It is scriping at  37 /72 pages...\n",
      "It is scriping at  38 /72 pages...\n",
      "It is scriping at  39 /72 pages...\n",
      "It is scriping at  40 /72 pages...\n",
      "It is scriping at  41 /72 pages...\n",
      "It is scriping at  42 /72 pages...\n",
      "It is scriping at  43 /72 pages...\n",
      "It is scriping at  44 /72 pages...\n",
      "It is scriping at  45 /72 pages...\n",
      "It is scriping at  46 /72 pages...\n",
      "It is scriping at  47 /72 pages...\n",
      "It is scriping at  48 /72 pages...\n",
      "It is scriping at  49 /72 pages...\n",
      "It is scriping at  50 /72 pages...\n",
      "It is scriping at  51 /72 pages...\n",
      "It is scriping at  52 /72 pages...\n",
      "It is scriping at  53 /72 pages...\n",
      "It is scriping at  54 /72 pages...\n",
      "It is scriping at  55 /72 pages...\n",
      "It is scriping at  56 /72 pages...\n",
      "It is scriping at  57 /72 pages...\n",
      "It is scriping at  58 /72 pages...\n",
      "It is scriping at  59 /72 pages...\n",
      "It is scriping at  60 /72 pages...\n",
      "It is scriping at  61 /72 pages...\n",
      "It is scriping at  62 /72 pages...\n",
      "It is scriping at  63 /72 pages...\n",
      "It is scriping at  64 /72 pages...\n",
      "It is scriping at  65 /72 pages...\n",
      "It is scriping at  66 /72 pages...\n",
      "It is scriping at  67 /72 pages...\n",
      "It is scriping at  68 /72 pages...\n",
      "It is scriping at  69 /72 pages...\n",
      "It is scriping at  70 /72 pages...\n",
      "It is scriping at  71 /72 pages...\n",
      "It is scriping at  72 /72 pages...\n"
     ]
    }
   ],
   "source": [
    "house_infos = []\n",
    "\n",
    "time.sleep(5)\n",
    "city = driver.find_elements_by_tag_name(\"input\")[0]\n",
    "city.clear()\n",
    "city.send_keys(\"Toronto, ON\")\n",
    "\n",
    "time.sleep(5)\n",
    "driver.find_elements_by_tag_name(\"button\")[0].click()\n",
    "\n",
    "time.sleep(5)\n",
    "driver.find_elements_by_css_selector(\"path.zoom-minus\")[0].click()\n",
    "\n",
    "time.sleep(5)\n",
    "driver.find_elements_by_css_selector(\"path.zoom-minus\")[0].click()\n",
    "\n",
    "time.sleep(15)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "pages = str(soup.find_all(\"em\")[0]).lstrip(\"<em>\").rstrip(\"</em>\")\n",
    "pages = int(pages[pages.index('f')+2::])\n",
    "\n",
    "time.sleep(5)\n",
    "for i in range(pages):\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    listing_houses = soup.find_all(\"div\", class_=\"card-wrapper\")\n",
    "    \n",
    "    print(\"It is scriping at \" + str(i+1) + \"/\" + str(pages) + \" pages...\")\n",
    "    \n",
    "    for house in listing_houses:\n",
    "    \n",
    "        house_address = house.find_all(\"div\", {\"class\": \"street\"})\n",
    "        house_details = house.find_all(\"div\", {\"class\": \"details\"})\n",
    "        house_price = house.find_all(\"div\", {\"class\": \"price\"})\n",
    "\n",
    "        house_info = [house_address, house_details, house_price]\n",
    "        house_infos.append(list(map(lambda x: [info.getText() for info in x][0].strip(\"\\n\"),house_info)))\n",
    "    \n",
    "\n",
    "    \n",
    "    if i < pages-1:\n",
    "        breakcount = 0\n",
    "        time.sleep(5)\n",
    "        while driver.find_elements_by_css_selector(\"a.icon-arrow-right-open\") == False:\n",
    "            time.sleep(5)\n",
    "            breakcount += 1\n",
    "            if breakcount > 20:\n",
    "                break\n",
    "        driver.find_elements_by_css_selector(\"a.icon-arrow-right-open\")[0].click()\n",
    "      \n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form the dataframe of the states around Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data = house_infos)\n",
    "df.columns = ['Address', 'Home Type', 'Price']\n",
    "\n",
    "#Make price value to be integer\n",
    "df[\"Price\"] = [str(x[2].replace(\"$\",\"\").replace(\"\\n\",\"\").replace(\",\", \"\").replace(\" \",\"\")) for x in house_infos]\n",
    "\n",
    "#Make adress to be available searching location on Nominatim \n",
    "addr_series = df[\"Address\"].copy()\n",
    "for idx, address in enumerate(addr_series):\n",
    "    if \"-\" in address:\n",
    "        minusidx = address.index('-')\n",
    "        addr_series.set_value(idx, addr_series[idx][minusidx+1::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add postal code to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Getting postal code from geocoder.canadapost\n",
    "postalcodes = []\n",
    "addr_index = 1\n",
    "total_addr = df.shape[0]\n",
    "for address in addr_series.tolist():\n",
    "    g = geocoder.canadapost(address + \", Toronto\")\n",
    "    postalcodes.append(g.postal[0:3])\n",
    "    addr_index+=1\n",
    "df.insert(0, \"Postal Code\", postalcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"Toronto_Sur_Estate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to a lot of estates are not belonged to Toronto, drop the estates that not related to this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unique_postal = pd.DataFrame({'Postal Code':df[\"Postal Code\"].unique()})\n",
    "\n",
    "lat_lng_coords = pd.read_csv('Geospatial_Coordinates.csv')\n",
    "lat_lng_coords_list = lat_lng_coords['Postal Code'].tolist()\n",
    "#Delete the postal code not belong to Toronto\n",
    "postal_notToronto = []\n",
    "for postalcode in unique_postal['Postal Code']:\n",
    "    if postalcode not in lat_lng_coords_list:\n",
    "        unique_postal = unique_postal[unique_postal['Postal Code'] != postalcode]\n",
    "        postal_notToronto.append(postalcode)\n",
    "unique_postal = unique_postal.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_Tor_Est = df.copy()\n",
    "new_addr_series = addr_series.copy()\n",
    "\n",
    "for postalcode in postal_notToronto:\n",
    "    df_Tor_Est = df_Tor_Est[df_Tor_Est['Postal Code'] != postalcode]\n",
    "    new_addr_series = new_addr_series[new_addr]\n",
    "\n",
    "df_Tor_Est = df_Tor_Est.reset_index()\n",
    "df_Tor_Est.to_csv(\"Toronto_Estate.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Nominatim to find the coordinat of each estate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim()\n",
    "latlon = []\n",
    "new_addr_series = df_Tor_Est[\"Address\"].copy()\n",
    "for idx, address in enumerate(new_addr_series):\n",
    "    if \"-\" in address:\n",
    "        minusidx = address.index('-')\n",
    "        new_addr_series.set_value(idx, new_addr_series[idx][minusidx+1::])\n",
    "addr_list = new_addr_series.tolist()\n",
    "for address in addr_list:\n",
    "    location = geolocator.geocode(address + \", Ontario\")\n",
    "    if location != None:\n",
    "        latlon.append([location.latitude,location.longitude])\n",
    "    else:\n",
    "        latlon.append([np.nan, np.nan])\n",
    "    \n",
    "lat_lon_df = pd.DataFrame(data = latlon)\n",
    "lat_lon_df.columns = [\"Latitude\", \"Longitude\"]\n",
    "df_Tor_Est = pd.concat([df_Tor_Est, lat_lon_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to there are some invalid address that need to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1589, 6)\n"
     ]
    }
   ],
   "source": [
    "df_Tor_Est.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1519, 6)\n"
     ]
    }
   ],
   "source": [
    "df_Tor_Est = df_Tor_Est[np.invert(np.isnan(df_Tor_Est['Latitude']))]\n",
    "df_Tor_Est.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_Tor_Est.to_csv(\"Toronto_Estate.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update unique postal codes coordinate for foursquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "postalcodes = []\n",
    "for address in addr_series:\n",
    "    g = geocoder.canadapost(address + \", Toronto\")\n",
    "    postalcodes.append(g.postal[0:3])\n",
    "df_postal = pd.DataFrame({'PostalCode': postalcodes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated_unique_postal = pd.DataFrame({'Postal Code':df_Tor_Est[\"Postal Code\"].unique()})\n",
    "\n",
    "postal_codes_coord = pd.Series(map(lambda x: lat_lng_coords[['Latitude', 'Longitude']][lat_lng_coords['Postal Code'] == x].values.tolist()[0], updated_unique_postal['Postal Code']))\n",
    "postal_codes_coord_df = pd.DataFrame(columns=['Latitude', 'Longitude'], data = list(postal_codes_coord))\n",
    "postal_codes_coord_df = pd.concat([updated_unique_postal, postal_codes_coord_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use foursquare to access nearby venues\n",
    "due to there are much of postal code areas, I spent two days to fininsh the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes,CLIENT_ID, CLIENT_SECRET, VERSION,LIMIT, radius):\n",
    "    \n",
    "    venues_list=[]\n",
    "    df_index = 1\n",
    "    totalnum = names.shape[0]\n",
    "    \n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        \n",
    "        df_index+=1    \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Postal Code', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Foursquare ID and Scecretc\n",
    "CLIENT_ID = '' # your Foursquare ID\n",
    "CLIENT_SECRET = '' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "LIMIT = 83\n",
    "\n",
    "Toronto_venues_1 = getNearbyVenues(names=postal_codes_coord_df['Postal Code'][0:30],\n",
    "                                   latitudes=postal_codes_coord_df['Latitude'][0:30],\n",
    "                                   longitudes=postal_codes_coord_df['Longitude'][0:30],\n",
    "                                   CLIENT_ID=CLIENT_ID,\n",
    "                                   CLIENT_SECRET=CLIENT_SECRET,\n",
    "                                   VERSION=VERSION,\n",
    "                                   LIMIT=LIMIT,\n",
    "                                   radius=2000\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Foursquare ID and Scecretc\n",
    "CLIENT_ID = '' # your Foursquare ID\n",
    "CLIENT_SECRET = '' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "LIMIT = 86\n",
    "\n",
    "Toronto_venues_2 = getNearbyVenues(names=postal_codes_coord_df['Postal Code'][30::],\n",
    "                                   latitudes=postal_codes_coord_df['Latitude'][30::],\n",
    "                                   longitudes=postal_codes_coord_df['Longitude'][30::],\n",
    "                                   CLIENT_ID=CLIENT_ID,\n",
    "                                   CLIENT_SECRET=CLIENT_SECRET,\n",
    "                                   VERSION=VERSION,\n",
    "                                   LIMIT=LIMIT,\n",
    "                                   radius=2000\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4876, 8)\n"
     ]
    }
   ],
   "source": [
    "Toronto_venues = pd.concat([Toronto_venues_1, Toronto_venues_2], axis=0, sort=False).reset_index()\n",
    "Toronto_venues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Toronto_venues.to_csv(\"Toronto_Venues.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
